#!/bin/bash

echo "ðŸš€ Starte Predictive Maintenance MVP..."

echo "1ï¸âƒ£ Starte Docker Compose..."
docker compose -f docker-compose.yml up --build -d

# Warte auf Kafka-VerfÃ¼gbarkeit
echo "â³ Warte auf Kafka..."
until docker compose exec kafka kafka-topics.sh --bootstrap-server localhost:9092 --list > /dev/null 2>&1; do
  sleep 2
done
echo "âœ… Kafka ist verfÃ¼gbar!"

echo "3ï¸âƒ£ Starte Kafka-Datenstrom..."
python3 src/ingestion/stream_simulator.py &

sleep 5
# Warte auf Spark
echo "â³ Warte auf Spark..."
until docker compose exec spark ls /app > /dev/null 2>&1; do
  sleep 2
done
echo "âœ… Spark ist verfÃ¼gbar!"

# Gib Spark noch einen Moment nach dem Start
sleep 3

echo "âš™ï¸  Starte Spark Streaming Job..."
docker compose exec spark spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,io.delta:delta-core_2.12:2.4.0 \
  /app/src/processing/stream_job.py


sleep 5
echo "5ï¸âƒ£ Starte Tagesaggregation..."
docker compose exec spark spark-submit /app/src/processing/batch_job.py

echo "6ï¸âƒ£ Trainiere Modell..."
docker compose exec spark python3 /app/src/model/train_model.py > logs/train_model.log 2>&1

echo "7ï¸âƒ£ Evaluiere Modell..."
docker compose exec spark python3 /app/src/model/evaluate_model.py

echo "8ï¸âƒ£ Registriere bestes Modell..."
docker compose exec spark python3 /app/src/model/register_model.py

echo "9ï¸âƒ£ Teste API-Vorhersage..."
python3 src/serving/predict.py

# Ã–ffne UIs automatisch im Browser (macOS, Linux mit xdg-open oder Windows WSL-kompatibel)
MLFLOW_UI="http://localhost:5050"
FASTAPI_UI="http://localhost:8000/docs"

echo "ðŸŒ Ã–ffne MLflow UI: $MLFLOW_UI"
echo "ðŸŒ Ã–ffne FastAPI UI: $FASTAPI_UI"

if command -v open &> /dev/null; then
  open $MLFLOW_UI
  open $FASTAPI_UI
elif command -v xdg-open &> /dev/null; then
  xdg-open $MLFLOW_UI
  xdg-open $FASTAPI_UI
fi

echo "âœ… MVP vollstÃ¤ndig aktiv!"
